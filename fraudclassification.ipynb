{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mahyararani/fraudclassification?scriptVersionId=112021130\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#241571; background:#B5cae9; border:2px dashed #efe50b;\" role=\"tab\" aria-controls=\"goal\"><center></bold>Fraud Classification: An Explotation of Classification Models</center></h1>\n\n![Linear_Regression.jpg](https://www.frbservices.org/binaries/content/gallery/crsocms/news/fed360/070120/fraudclassifier-full.png)","metadata":{"execution":{"iopub.execute_input":"2022-11-06T15:43:38.72829Z","iopub.status.busy":"2022-11-06T15:43:38.727965Z","iopub.status.idle":"2022-11-06T15:43:38.736754Z","shell.execute_reply":"2022-11-06T15:43:38.735565Z","shell.execute_reply.started":"2022-11-06T15:43:38.728254Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer # Imputing missing values\nfrom imblearn.under_sampling import RandomUnderSampler # Class Imbalance\n\n# Numerical features selection\nfrom sklearn.decomposition import PCA \n\n# Categorical Features Selection\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n# Classification Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import naive_bayes\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\n# Model Optimization\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Metrics\nfrom sklearn import metrics\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.compose import ColumnTransformer\n\n# Other General Imports\nimport gc\n%matplotlib inline\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T02:50:51.402715Z","iopub.execute_input":"2022-11-25T02:50:51.403187Z","iopub.status.idle":"2022-11-25T02:50:51.422917Z","shell.execute_reply.started":"2022-11-25T02:50:51.403155Z","shell.execute_reply":"2022-11-25T02:50:51.420771Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading and Reducing the Size\n\nSince the data is big in size, we will use function to reduce its memory for fast processing and consuming less storage.","metadata":{}},{"cell_type":"code","source":"start = time.time()\n# Helper function\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-25T00:40:30.967408Z","iopub.execute_input":"2022-11-25T00:40:30.968647Z","iopub.status.idle":"2022-11-25T00:40:30.984183Z","shell.execute_reply.started":"2022-11-25T00:40:30.968577Z","shell.execute_reply":"2022-11-25T00:40:30.982829Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# loading train_transaction data\ntt = pd.read_csv('../input/ieeecis-fraud-detection/train_transaction.csv')\nprint(tt.shape)\ntt = reduce_mem_usage(tt)\n\n# loading train_transaction data\nti = pd.read_csv('../input/ieeecis-fraud-detection/train_identity.csv')\nprint(ti.shape)\nti = reduce_mem_usage(ti)\n\ntrain = pd.merge(tt, ti, how = 'left')\nprint('Train shape',train.shape)\n\ntrain.head()\n\ndel tt, ti","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-24T06:08:23.385053Z","iopub.execute_input":"2022-11-24T06:08:23.385576Z","iopub.status.idle":"2022-11-24T06:10:48.295716Z","shell.execute_reply.started":"2022-11-24T06:08:23.385536Z","shell.execute_reply":"2022-11-24T06:10:48.294027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading test_transaction data\nts = pd.read_csv('../input/ieeecis-fraud-detection/test_transaction.csv')\nprint(ts.shape)\nts = reduce_mem_usage(ts)\n\ntsi = pd.read_csv('../input/ieeecis-fraud-detection/test_identity.csv')\nprint(tsi.shape)\ntsi = reduce_mem_usage(tsi)\n\ntest = pd.merge(ts, tsi, how = 'left')\nprint('Test shape',test.shape)\ntest.head()\n\ndel ts, tsi","metadata":{"execution":{"iopub.status.busy":"2022-11-23T05:32:49.593867Z","iopub.execute_input":"2022-11-23T05:32:49.594228Z","iopub.status.idle":"2022-11-23T05:35:33.480634Z","shell.execute_reply.started":"2022-11-23T05:32:49.594195Z","shell.execute_reply":"2022-11-23T05:35:33.479153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding\n\n## Data Exploration","metadata":{}},{"cell_type":"code","source":"# Class imbalance check\nplt.pie(train.isFraud.value_counts(), labels=['Not Fraud', 'Fraud'], autopct='%0.1f%%')\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As one can expect, this is a class imbalance problem. Thus, the accuracy and specifity are not a good criteria to measure the models we want to apply later and we must change the sensitivity in our classification models and change it to 0.35 indstead of 0.5 or apply more novel methods dealing with imbalance dataset. \nWe will apply UnderSampling to deal with class imbalance in later steps. Let us understand the distribution of the timestamp column.","metadata":{}},{"cell_type":"markdown","source":"### TransactionDT","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.hist(train['TransactionDT'], label='Train', bins = 25, color=\"#005BBB\")\nplt.hist(test['TransactionDT'], label='Test', bins = 25, color= \"#FFD500\")\nplt.ylabel('Count')\nplt.title('Transaction Timestamp')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The difference train.min() and test.max() is x = 34214345 - 86400 = 34127945 but we don't know is it in seconds,minutes or hours.\n\nIf it is in hours then the dataset timespan will be x/(24*365) = 3895.884132 years which is not possible.\n\nif it is in minutes then the dataset timespan will be x/(60*24*365) = 64.931402 years which is not possible because Vesta Corporation was founded in 1995 so they can have at most 24 years of data.\n\nIf it is in seconds then dataset timespan will be x/(3600*24*365) = 1.0821 years which seems reasonable to me.","metadata":{}},{"cell_type":"markdown","source":"**Time span of the total dataset is 394.9993634259259 days**\n\n**Time span of Train dataset is  181.99920138888888 days**\n\n**Time span of Test dataset is  182.99908564814814 days**\n\n**The gap between train and test is 30.00107638888889 days**","metadata":{}},{"cell_type":"code","source":"\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train['TransactionDT'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionDT', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'isFraud'\ncor = np.corrcoef(train['TransactionDT'], train[i])[0,1]\ntrain.loc[train['isFraud'] == 0].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\ntrain.loc[train['isFraud'] == 1].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\nplt.legend()\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transation Amount","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train['TransactionAmt'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ProductCD Feature","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.countplot(x=\"ProductCD\", ax=ax[0], hue = \"isFraud\", data=train)\nax[0].set_title('ProductCD train', fontsize=14)\nsns.countplot(x=\"ProductCD\", ax=ax[1], data=test)\nax[1].set_title('ProductCD test', fontsize=14)\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cards Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.suptitle('Card 4 Distributions', fontsize=18)\n\nplt.subplot(221)\ng1 = sns.countplot(x='card4', data=train)\ng1 = sns.countplot(x='card4', data=train)\ng1.set_title(\"Card4 Distribution\", fontsize=12)\ng1.set_ylabel(\"Count\", fontsize=12)\n\nplt.subplot(212)\ng2 = sns.boxenplot(x='card4', y='TransactionAmt', hue='isFraud', \n              data=train[train['TransactionAmt'] <= 2000] )\ng2.set_title(\"Card 4 Distribuition by ProductCD and Target\", fontsize=12)\ng2.set_xlabel(\"Card4 Category Names\", fontsize=12)\ng2.set_ylabel(\"Transaction Values\", fontsize=12)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.suptitle('Card 6 Distributions', fontsize=18)\n\nplt.subplot(221)\ng = sns.countplot(x='card6', data=train)\ng.set_title(\"Card6 Distribution\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\n\nplt.subplot(212)\ng1 = sns.boxenplot(x='card6', y='TransactionAmt', hue='isFraud', \n              data=train[train['TransactionAmt'] <= 2000] )\ng1.set_title(\"Card 6 Distribuition by ProductCD and Target\", fontsize=12)\ng1.set_xlabel(\"Card6 Category Names\", fontsize=12)\ng1.set_ylabel(\"Transaction Values\", fontsize=12)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cards = ['card1', 'card2', 'card3', 'card5']\n\nfor col in cards:\n    fig = plt.figure(figsize = (9,6))\n    ax = fig.gca()\n    feature = train[col]\n    feature.hist(bins = 100, ax=ax)\n    ax.axvline(feature.mean(), color = 'magenta', linestyle = 'dashed', linewidth = 2)\n    ax.axvline(feature.median(), color= 'cyan', linestyle = 'dashed', linewidth = 2)\n    ax.set_title(col)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Addr1 and Addr2 Features","metadata":{}},{"cell_type":"code","source":"print(\"Card Features Quantiles: \")\nprint(train[['addr1', 'addr2']].quantile([0.01, 0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975, 0.99]))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C1-C14 Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nc_features = list(train.columns[16:30])\nuniques = [len(train[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### D1-D15 Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nd_features = list(train.columns[30:45])\nuniques = [len(train[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### V1-V339 Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train.columns[54:170])\nuniques = [len(train[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Device Type in Train Identity","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(x=\"DeviceType\", data=train)\nax.set_title('DeviceType', fontsize=14)\nsns.set(rc={\"figure.figsize\": (3,4)})\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Device Information in Train Identity","metadata":{}},{"cell_type":"code","source":"print (\"Unique Devices = \",train['DeviceInfo'].nunique())\ntrain['DeviceInfo'].value_counts().head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"def summary(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n\n    return summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering 1","metadata":{}},{"cell_type":"markdown","source":"### Handling and Genearating Features","metadata":{}},{"cell_type":"code","source":"def Devices(df):\n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n    df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n    df = df.drop(['DeviceInfo'], axis = 1)\n    \n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n    df = df.drop(['id_30'], axis = 1)\n    \n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]\n    df = df.drop(['id_31'], axis = 1)\n\n    df['screen_width'] = df['id_33'].str.split('x', expand=True)[0]\n    df['screen_height'] = df['id_33'].str.split('x', expand=True)[1]\n    df = df.drop(['id_33'], axis = 1)\n\n    df['id_34'] = df['id_34'].str.split(':', expand=True)[1]\n    df['id_23'] = df['id_23'].str.split(':', expand=True)[1]\n    df = df.drop(['id_34', 'id_23'], axis = 1)\n\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-11-25T01:23:08.523007Z","iopub.execute_input":"2022-11-25T01:23:08.523427Z","iopub.status.idle":"2022-11-25T01:23:08.546635Z","shell.execute_reply.started":"2022-11-25T01:23:08.523396Z","shell.execute_reply":"2022-11-25T01:23:08.544962Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train = Devices(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with Missing Data","metadata":{}},{"cell_type":"markdown","source":"### Identifying the uniqueness of categorical features","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates(inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to drop the columns with more than 80% missing values baed on [THIS](https://medium.com/analytics-vidhya/how-to-handle-missing-values-cbd03fb79ef8) paper.\n\nLet us now fill all the missing values. For numerical columns, we will use median value and for categorical column, we will use the most frequent category to fill the missing values.","metadata":{}},{"cell_type":"code","source":"y = train['isFraud']\nTransactionID = train['TransactionID']\nTransactionDT = train['TransactionDT']\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:10:48.298471Z","iopub.execute_input":"2022-11-24T06:10:48.299718Z","iopub.status.idle":"2022-11-24T06:10:48.395273Z","shell.execute_reply.started":"2022-11-24T06:10:48.299654Z","shell.execute_reply":"2022-11-24T06:10:48.393683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping columns with more than 80% missing values \nprint(\"Shape before dropping features more than 20% missing values: \", train.shape)\nmv = train.isnull().sum()/len(train)\ntrain = train.drop(columns=mv[mv>0.8].index)\n\nprint(\"Shape after dropping features more than 20% missing values: \", train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:11:46.618923Z","iopub.execute_input":"2022-11-24T06:11:46.619516Z","iopub.status.idle":"2022-11-24T06:11:49.165344Z","shell.execute_reply.started":"2022-11-24T06:11:46.619465Z","shell.execute_reply":"2022-11-24T06:11:49.164193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dropna(how= 'all', axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:11:49.167454Z","iopub.execute_input":"2022-11-24T06:11:49.167819Z","iopub.status.idle":"2022-11-24T06:11:52.375626Z","shell.execute_reply.started":"2022-11-24T06:11:49.167788Z","shell.execute_reply":"2022-11-24T06:11:52.374347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on metadata Categorical Featurs are:\n\n* ProductCD\n* emaildomain\n* card4 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\nHowever, by filtering the features based on the data type (obj) we have more categorical features.","metadata":{}},{"cell_type":"code","source":"train = train.drop(['isFraud', 'TransactionID', 'TransactionDT'], axis = 1)\n# Filtering numerical data\nnum_df = train.select_dtypes(include=np.number)\nprint(num_df.shape)\n\n# Filtering categorical data\ncat_df = train.select_dtypes(exclude=np.number)\nprint(cat_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:12:01.52459Z","iopub.execute_input":"2022-11-24T06:12:01.525022Z","iopub.status.idle":"2022-11-24T06:12:02.947396Z","shell.execute_reply.started":"2022-11-24T06:12:01.524986Z","shell.execute_reply":"2022-11-24T06:12:02.946147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:12:02.951123Z","iopub.execute_input":"2022-11-24T06:12:02.951539Z","iopub.status.idle":"2022-11-24T06:12:02.989566Z","shell.execute_reply.started":"2022-11-24T06:12:02.951506Z","shell.execute_reply":"2022-11-24T06:12:02.988413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:12:02.991052Z","iopub.execute_input":"2022-11-24T06:12:02.991567Z","iopub.status.idle":"2022-11-24T06:12:03.027085Z","shell.execute_reply.started":"2022-11-24T06:12:02.99152Z","shell.execute_reply":"2022-11-24T06:12:03.025414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:12:03.258354Z","iopub.execute_input":"2022-11-24T06:12:03.258739Z","iopub.status.idle":"2022-11-24T06:12:03.443719Z","shell.execute_reply.started":"2022-11-24T06:12:03.25871Z","shell.execute_reply":"2022-11-24T06:12:03.442425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values by median for numerical columns \nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')\nnum_df = pd.DataFrame(imp_median.fit_transform(num_df), columns=num_df.columns)\nprint(num_df.shape)\n\n# Filling missing values by most frequent value for categorical columns\nimp_median = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ncat_df = pd.DataFrame(imp_median.fit_transform(cat_df), columns=cat_df.columns)\n\n    \ncat_df.isnull().sum()\nnum_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T06:12:06.74903Z","iopub.execute_input":"2022-11-24T06:12:06.750184Z","iopub.status.idle":"2022-11-24T06:12:42.643203Z","shell.execute_reply.started":"2022-11-24T06:12:06.750147Z","shell.execute_reply":"2022-11-24T06:12:42.641928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Features","metadata":{}},{"cell_type":"code","source":"scaled_num = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(num_df), columns = num_df.columns)\nscaled_num.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA for Numerical Features","metadata":{}},{"cell_type":"code","source":"#optimuadd_suffixm number of components\npca = PCA().fit(scaled_num)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"number of components\")\nplt.ylabel(\"Cumulative Rate of Variance\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final\npca = PCA(n_components = 0.95)\n\npca_fit = pca.fit_transform(scaled_num)\nnum_pca = pd.DataFrame(data = pca_fit)\npca.explained_variance_ratio_.sum()\ndel num_df, scaled_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_pca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Features","metadata":{}},{"cell_type":"markdown","source":"Due to we have too many values in some categories, we try to map them into new categories.","metadata":{}},{"cell_type":"code","source":"for x in cat_df.columns:\n    #printing unique values\n    print(x ,':', len(cat_df[x].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_df = pd.get_dummies(cat_df)\ncat_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tree-based Categorical Feature Selection","metadata":{}},{"cell_type":"code","source":"clf = ExtraTreesClassifier(n_estimators=100, criterion = 'entropy')\nclf = clf.fit(cat_df, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SelectFromModel(clf, prefit=True)\nfeature_idx = model.get_support()\nfeature_name = cat_df.columns[feature_idx]\n\ncat_new = pd.DataFrame(model.transform(cat_df), columns = feature_name)\ncat_new.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenating Numerical and Categorical Features ","metadata":{}},{"cell_type":"code","source":"# Concatinating numerical and categorical data\ndf = pd.concat([y, TransactionID, TransactionDT, num_pca, cat_new], axis=1)\ndf = pd.DataFrame(df)\n\n# Verifying missing values\nprint(f'Total missing values: {df.isnull().sum().sum()}')\nprint(df.shape)\ndf.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del  cat_new, num_pca, y\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = reduce_mem_usage(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This part is only for reducing time work on the clean dataset and apply the models**","metadata":{}},{"cell_type":"code","source":"df.to_csv('df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-22T20:39:45.47174Z","iopub.execute_input":"2022-11-22T20:39:45.472212Z","iopub.status.idle":"2022-11-22T20:39:45.491035Z","shell.execute_reply.started":"2022-11-22T20:39:45.472172Z","shell.execute_reply":"2022-11-22T20:39:45.489904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/dffraudclassification/df.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T00:41:09.435801Z","iopub.execute_input":"2022-11-25T00:41:09.436447Z","iopub.status.idle":"2022-11-25T00:41:25.982322Z","shell.execute_reply.started":"2022-11-25T00:41:09.436371Z","shell.execute_reply":"2022-11-25T00:41:25.980709Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  isFraud  TransactionID  TransactionDT        0        1  \\\n0           0        0        2987000          86400 -0.32370 -0.19540   \n1           1        0        2987001          86401 -0.36700  0.27000   \n2           2        0        2987002          86469 -0.07666 -0.15780   \n3           3        0        2987003          86499 -0.20450 -0.03568   \n4           4        0        2987004          86506 -0.39140 -0.13420   \n\n         2        3        4        5  ...  browser_id_31_firefox  \\\n0  0.16500  0.42720  0.04540 -0.18640  ...                    0.0   \n1  0.13180  0.18270  0.67330 -0.02573  ...                    0.0   \n2 -0.19960  0.11945  0.42100 -0.20010  ...                    0.0   \n3 -0.12870  0.85940 -0.01144 -0.22830  ...                    0.0   \n4 -0.10614  0.42360  0.66000 -0.24900  ...                    0.0   \n\n   version_id_31_58.0  version_id_31_61.0  version_id_31_62.0  \\\n0                 0.0                 0.0                 0.0   \n1                 0.0                 0.0                 0.0   \n2                 0.0                 0.0                 0.0   \n3                 0.0                 0.0                 0.0   \n4                 0.0                 0.0                 0.0   \n\n   version_id_31_63.0  version_id_31_64.0  version_id_31_65.0  \\\n0                 0.0                 0.0                 0.0   \n1                 0.0                 0.0                 0.0   \n2                 0.0                 0.0                 0.0   \n3                 0.0                 0.0                 0.0   \n4                 0.0                 0.0                 0.0   \n\n   version_id_31_66.0  version_id_31_generic  version_id_31_safari  \n0                 0.0                    0.0                   1.0  \n1                 0.0                    0.0                   1.0  \n2                 0.0                    0.0                   1.0  \n3                 0.0                    0.0                   1.0  \n4                 0.0                    0.0                   0.0  \n\n[5 rows x 119 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>isFraud</th>\n      <th>TransactionID</th>\n      <th>TransactionDT</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>...</th>\n      <th>browser_id_31_firefox</th>\n      <th>version_id_31_58.0</th>\n      <th>version_id_31_61.0</th>\n      <th>version_id_31_62.0</th>\n      <th>version_id_31_63.0</th>\n      <th>version_id_31_64.0</th>\n      <th>version_id_31_65.0</th>\n      <th>version_id_31_66.0</th>\n      <th>version_id_31_generic</th>\n      <th>version_id_31_safari</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2987000</td>\n      <td>86400</td>\n      <td>-0.32370</td>\n      <td>-0.19540</td>\n      <td>0.16500</td>\n      <td>0.42720</td>\n      <td>0.04540</td>\n      <td>-0.18640</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>2987001</td>\n      <td>86401</td>\n      <td>-0.36700</td>\n      <td>0.27000</td>\n      <td>0.13180</td>\n      <td>0.18270</td>\n      <td>0.67330</td>\n      <td>-0.02573</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>2987002</td>\n      <td>86469</td>\n      <td>-0.07666</td>\n      <td>-0.15780</td>\n      <td>-0.19960</td>\n      <td>0.11945</td>\n      <td>0.42100</td>\n      <td>-0.20010</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2987003</td>\n      <td>86499</td>\n      <td>-0.20450</td>\n      <td>-0.03568</td>\n      <td>-0.12870</td>\n      <td>0.85940</td>\n      <td>-0.01144</td>\n      <td>-0.22830</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>2987004</td>\n      <td>86506</td>\n      <td>-0.39140</td>\n      <td>-0.13420</td>\n      <td>-0.10614</td>\n      <td>0.42360</td>\n      <td>0.66000</td>\n      <td>-0.24900</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 119 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T00:41:04.122431Z","iopub.status.idle":"2022-11-25T00:41:04.122999Z","shell.execute_reply.started":"2022-11-25T00:41:04.122728Z","shell.execute_reply":"2022-11-25T00:41:04.122753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Imbalance","metadata":{}},{"cell_type":"markdown","source":"We applied udersampling to decrease the modeling time and better identifieying the isFraud target feature.","metadata":{}},{"cell_type":"code","source":"y = df['isFraud']\nX = df.drop(['isFraud', 'Unnamed: 0', 'TransactionID', 'TransactionDT'], axis = 1)\nX_array=np.array(X)\n\nrus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\nprint(X_resampled.shape, y_resampled.shape)\n\npd.value_counts(y_resampled)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T00:41:04.125502Z","iopub.status.idle":"2022-11-25T00:41:04.126179Z","shell.execute_reply.started":"2022-11-25T00:41:04.125896Z","shell.execute_reply":"2022-11-25T00:41:04.125922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_df = pd.DataFrame(X_resampled, columns = X.columns)\ny_df = pd.DataFrame(y_resampled)\ndf = pd.concat([X_df, y_df], axis= 1)\ndf.shape \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T05:42:06.305239Z","iopub.execute_input":"2022-11-24T05:42:06.305926Z","iopub.status.idle":"2022-11-24T05:42:06.344821Z","shell.execute_reply.started":"2022-11-24T05:42:06.305884Z","shell.execute_reply":"2022-11-24T05:42:06.343572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class imbalance check\nplt.pie(y_df.value_counts(), labels=['Not Fraud', 'Fraud'], autopct='%0.1f%%')\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T05:42:06.346544Z","iopub.execute_input":"2022-11-24T05:42:06.346935Z","iopub.status.idle":"2022-11-24T05:42:06.498565Z","shell.execute_reply.started":"2022-11-24T05:42:06.346906Z","shell.execute_reply":"2022-11-24T05:42:06.497247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape ","metadata":{"execution":{"iopub.status.busy":"2022-11-24T05:42:06.500716Z","iopub.execute_input":"2022-11-24T05:42:06.502053Z","iopub.status.idle":"2022-11-24T05:42:06.510749Z","shell.execute_reply.started":"2022-11-24T05:42:06.502006Z","shell.execute_reply":"2022-11-24T05:42:06.509532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_df, y_df, X_resampled, y_resampled\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T05:42:06.512388Z","iopub.execute_input":"2022-11-24T05:42:06.513489Z","iopub.status.idle":"2022-11-24T05:42:06.656054Z","shell.execute_reply.started":"2022-11-24T05:42:06.513424Z","shell.execute_reply":"2022-11-24T05:42:06.65507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"y = df['isFraud']\nX = df.drop(['isFraud'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T05:42:06.658152Z","iopub.execute_input":"2022-11-24T05:42:06.659036Z","iopub.status.idle":"2022-11-24T05:42:06.684396Z","shell.execute_reply.started":"2022-11-24T05:42:06.658985Z","shell.execute_reply":"2022-11-24T05:42:06.683066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"nb_clf = naive_bayes.BernoulliNB()\n\nkf = KFold(n_splits = 4, shuffle = True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    nb_clf.fit(X_train, y_train)\n    y_pred = nb_clf.predict(X_test)\n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n\n\nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-22T04:03:59.946598Z","iopub.execute_input":"2022-11-22T04:03:59.946999Z","iopub.status.idle":"2022-11-22T04:04:00.972813Z","shell.execute_reply.started":"2022-11-22T04:03:59.946972Z","shell.execute_reply":"2022-11-22T04:04:00.970913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With GuassianNB the scores are relatively lower.","metadata":{}},{"cell_type":"code","source":"y_scores = nb_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nNBScore = metrics.auc(fpr, tpr)\nprint(NBScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Bernoulli Naive Bayes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:04:10.643642Z","iopub.execute_input":"2022-11-22T04:04:10.64402Z","iopub.status.idle":"2022-11-22T04:04:10.840768Z","shell.execute_reply.started":"2022-11-22T04:04:10.643991Z","shell.execute_reply":"2022-11-22T04:04:10.839745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.plot_confusion_matrix(nb_clf, X_test, y_test).ax_.grid(False)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:04:15.11166Z","iopub.execute_input":"2022-11-22T04:04:15.112056Z","iopub.status.idle":"2022-11-22T04:04:15.327675Z","shell.execute_reply.started":"2022-11-22T04:04:15.112023Z","shell.execute_reply":"2022-11-22T04:04:15.325801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"\nlg_clf = LogisticRegression(solver=\"liblinear\")\n\nkf = KFold(n_splits = 4, shuffle = True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    lg_clf.fit(X_train, y_train)\n    y_pred = lg_clf.predict(X_test)\n    \n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n\nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:04:29.535367Z","iopub.execute_input":"2022-11-22T04:04:29.535759Z","iopub.status.idle":"2022-11-22T04:04:35.078865Z","shell.execute_reply.started":"2022-11-22T04:04:29.535724Z","shell.execute_reply":"2022-11-22T04:04:35.076665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = lg_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nLGScore = metrics.auc(fpr, tpr)\nprint(LGScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Logistic Regression')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:04:38.165832Z","iopub.execute_input":"2022-11-22T04:04:38.16622Z","iopub.status.idle":"2022-11-22T04:04:38.323123Z","shell.execute_reply.started":"2022-11-22T04:04:38.166187Z","shell.execute_reply":"2022-11-22T04:04:38.322468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN Classifier\n### Tuning","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nerror_rate = []\nfor i in range(1,10):\n    knn = KNeighborsClassifier(n_neighbors=i, weights='distance')\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(metrics.precision_score(y_test, pred_i))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,10), error_rate,color='g', linestyle='dashed', marker='o', markerfacecolor='b', markersize=5)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:04:43.314554Z","iopub.execute_input":"2022-11-22T04:04:43.315288Z","iopub.status.idle":"2022-11-22T04:05:27.742786Z","shell.execute_reply.started":"2022-11-22T04:04:43.315257Z","shell.execute_reply":"2022-11-22T04:05:27.741494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_clf = KNeighborsClassifier(n_neighbors=8, weights='distance')\n\nkf = KFold(n_splits = 4, shuffle = True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    knn_clf.fit(X_train, y_train)\n    y_pred = knn_clf.predict(X_test)\n    \n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n    \n\nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:05:40.071837Z","iopub.execute_input":"2022-11-22T04:05:40.072313Z","iopub.status.idle":"2022-11-22T04:06:02.879402Z","shell.execute_reply.started":"2022-11-22T04:05:40.072274Z","shell.execute_reply":"2022-11-22T04:06:02.877959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = knn_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nKNScore = metrics.auc(fpr, tpr)\nprint(KNScore)\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- K-Nearest neighbors')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:06:28.513781Z","iopub.execute_input":"2022-11-22T04:06:28.514175Z","iopub.status.idle":"2022-11-22T04:06:34.282148Z","shell.execute_reply.started":"2022-11-22T04:06:28.514141Z","shell.execute_reply":"2022-11-22T04:06:34.281377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machine (SVM)","metadata":{}},{"cell_type":"code","source":"svm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\ny_pred = svm_clf.predict(X_test)\n\nacc = metrics.accuracy_score(y_test, y_pred)\nprec = metrics.precision_score(y_test, y_pred, average=\"weighted\")\nrec = metrics.recall_score(y_test, y_pred, average=\"weighted\")\nf1 = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n\nprint(\"acc: {:.4f}\\nprec: {:.4f}\\nrec: {:.4f}\\nf1: {:.4f}\".format(acc, prec, rec, f1))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:08:43.224074Z","iopub.execute_input":"2022-11-22T04:08:43.224489Z","iopub.status.idle":"2022-11-22T04:09:50.36897Z","shell.execute_reply.started":"2022-11-22T04:08:43.224445Z","shell.execute_reply":"2022-11-22T04:09:50.367082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nSVScore= metrics.auc(fpr, tpr)\nprint(SVScore)\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Support Vector Machine')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:08:14.095141Z","iopub.execute_input":"2022-11-22T04:08:14.095644Z","iopub.status.idle":"2022-11-22T04:08:14.249547Z","shell.execute_reply.started":"2022-11-22T04:08:14.095612Z","shell.execute_reply":"2022-11-22T04:08:14.24825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:14:13.224088Z","iopub.execute_input":"2022-11-22T04:14:13.224483Z","iopub.status.idle":"2022-11-22T04:14:13.466673Z","shell.execute_reply.started":"2022-11-22T04:14:13.22445Z","shell.execute_reply":"2022-11-22T04:14:13.465059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"dt_clf = DecisionTreeClassifier(criterion = 'entropy')\n\nkf = KFold(n_splits = 4, shuffle = True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    dt_clf.fit(X_train, y_train)\n    y_pred = dt_clf.predict(X_test)\n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n    \nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:14:14.541628Z","iopub.execute_input":"2022-11-22T04:14:14.541985Z","iopub.status.idle":"2022-11-22T04:14:25.540627Z","shell.execute_reply.started":"2022-11-22T04:14:14.541959Z","shell.execute_reply":"2022-11-22T04:14:25.539358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = dt_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nDTScore = metrics.auc(fpr, tpr)\nprint(DTScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Decision Tree')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-22T04:14:25.542496Z","iopub.execute_input":"2022-11-22T04:14:25.542789Z","iopub.status.idle":"2022-11-22T04:14:25.694721Z","shell.execute_reply.started":"2022-11-22T04:14:25.542764Z","shell.execute_reply":"2022-11-22T04:14:25.692943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(criterion = 'entropy')\n\nkf = KFold(n_splits = 4, shuffle = True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    rf_clf.fit(X_train, y_train)\n    y_pred = rf_clf.predict(X_test)\n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n    \nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:14:45.869585Z","iopub.execute_input":"2022-11-22T04:14:45.869967Z","iopub.status.idle":"2022-11-22T04:15:57.68252Z","shell.execute_reply.started":"2022-11-22T04:14:45.869936Z","shell.execute_reply":"2022-11-22T04:15:57.681567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = rf_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nRFScore = metrics.auc(fpr, tpr)\nprint(RFScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Random Forest')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:15:57.683968Z","iopub.execute_input":"2022-11-22T04:15:57.684243Z","iopub.status.idle":"2022-11-22T04:15:58.055179Z","shell.execute_reply.started":"2022-11-22T04:15:57.684218Z","shell.execute_reply":"2022-11-22T04:15:58.054409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MLP Classifier","metadata":{}},{"cell_type":"markdown","source":"### Tuning","metadata":{}},{"cell_type":"code","source":"parameter_space = {\n    'max_iter': [500, 1000, 2000],\n    'hidden_layer_sizes': [(10, ), (20,)],\n    'activation': ['tanh', 'relu'],\n    'alpha': [0.01, 0.1, 0.5, 0.9],\n}\nMLP_clf = GridSearchCV(MLPClassifier(), parameter_space, n_jobs=-1, cv=2)\nMLP_clf.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:16:06.255805Z","iopub.execute_input":"2022-11-22T04:16:06.256206Z","iopub.status.idle":"2022-11-22T04:25:55.195213Z","shell.execute_reply.started":"2022-11-22T04:16:06.256174Z","shell.execute_reply":"2022-11-22T04:25:55.194513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best parameters found:\\n', MLP_clf.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:26:54.626289Z","iopub.execute_input":"2022-11-22T04:26:54.62669Z","iopub.status.idle":"2022-11-22T04:26:54.633186Z","shell.execute_reply.started":"2022-11-22T04:26:54.626649Z","shell.execute_reply":"2022-11-22T04:26:54.631891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits =4, shuffle=True, random_state=0)\n\nacc, prec, rec, f1 = [], [], [], []\n\nfor train_idx, test_idx in kfold.split(df):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    \n    X_scaler = preprocessing.MinMaxScaler()\n    X_train = X_scaler.fit_transform(X_train)\n    \n    mlp_clf = MLPClassifier(hidden_layer_sizes=(10,),\n                            max_iter=500,\n                            activation=\"relu\",\n                            alpha= 0.5)\n        \n    \n    mlp_clf.fit(X_train, y_train)\n    \n    y_pred = mlp_clf.predict(X_test)\n    \n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average= 'weighted')]\n    prec += [metrics.precision_score(y_pred, y_test, average= 'weighted')]\n    f1 += [metrics.f1_score(y_pred, y_test, average= 'weighted')]\n    \nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall    = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1        = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:28:06.280918Z","iopub.execute_input":"2022-11-22T04:28:06.28127Z","iopub.status.idle":"2022-11-22T04:28:30.367136Z","shell.execute_reply.started":"2022-11-22T04:28:06.281242Z","shell.execute_reply":"2022-11-22T04:28:30.366088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = mlp_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nMLPScore = metrics.auc(fpr, tpr)\nprint(MLPScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Random Forest')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:29:57.233329Z","iopub.execute_input":"2022-11-22T04:29:57.23366Z","iopub.status.idle":"2022-11-22T04:29:57.395635Z","shell.execute_reply.started":"2022-11-22T04:29:57.233635Z","shell.execute_reply":"2022-11-22T04:29:57.394625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bagging Classifier","metadata":{}},{"cell_type":"code","source":"bc_clf = BaggingClassifier()\n\nkf = KFold(n_splits = 4, shuffle = True, random_state =0)\nacc, prec, rec, f1 = [], [], [], []\n\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    bc_clf.fit(X_train, y_train)\n    y_pred = bc_clf.predict(X_test)\n    \n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n\n\nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:30:07.289756Z","iopub.execute_input":"2022-11-22T04:30:07.290195Z","iopub.status.idle":"2022-11-22T04:31:14.021996Z","shell.execute_reply.started":"2022-11-22T04:30:07.290156Z","shell.execute_reply":"2022-11-22T04:31:14.02101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = bc_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nBCScore = metrics.auc(fpr, tpr)\nprint(BCScore)\n\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Bagging Classifier')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-22T04:31:14.023462Z","iopub.execute_input":"2022-11-22T04:31:14.023731Z","iopub.status.idle":"2022-11-22T04:31:14.202235Z","shell.execute_reply.started":"2022-11-22T04:31:14.023707Z","shell.execute_reply":"2022-11-22T04:31:14.201416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.plot_confusion_matrix(bc_clf, X_test, y_test).ax_.grid(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Classifier\n### Tuning","metadata":{}},{"cell_type":"code","source":"#gb_clf = GradientBoostingClassifier(learning_rate=0.001, max_depth= 8,\n#                                    max_features=5, n_estimators= 100, random_state=0)\ngb_clf = GradientBoostingClassifier()\nkf = KFold(n_splits = 4, shuffle = True)\nacc, prec, rec, f1 = [], [], [], []\n\nfor train_index, test_index in kf.split(df):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    gb_clf.fit(X_train, y_train)\n    y_pred = gb_clf.predict(X_test)\n    \n    acc += [metrics.accuracy_score(y_pred, y_test)]\n    rec += [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\n    prec += [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\n    f1 += [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n\nprint(\"accuracy    = {:.4f} ±{:.4f}\".format(np.mean(acc), np.std(acc)))\nprint(\"recall      = {:.4f} ±{:.4f}\".format(np.mean(rec), np.std(rec)))\nprint(\"precision   = {:.4f} ±{:.4f}\".format(np.mean(prec), np.std(prec)))\nprint(\"f1          = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:31:14.203595Z","iopub.execute_input":"2022-11-22T04:31:14.203937Z","iopub.status.idle":"2022-11-22T04:34:22.405993Z","shell.execute_reply.started":"2022-11-22T04:31:14.203888Z","shell.execute_reply":"2022-11-22T04:34:22.404665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = gb_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nGBScore = metrics.auc(fpr, tpr)\nprint(GBScore)\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Gradient Boosting')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:34:22.408259Z","iopub.execute_input":"2022-11-22T04:34:22.408593Z","iopub.status.idle":"2022-11-22T04:34:22.559958Z","shell.execute_reply.started":"2022-11-22T04:34:22.408564Z","shell.execute_reply":"2022-11-22T04:34:22.558795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing Models","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nplt.figure(figsize=(8, 4))\nplt.plot(['N_Bayes', 'Logistic_R', 'KNN', 'SVM', 'D_Tree', 'R_Forest', 'MLP', 'Bagging', 'G_Boost'],\n         [NBScore, LGScore, KNScore, SVScore, DTScore, RFScore, MLPScore, BCScore, GBScore], 'ro', markersize=10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:34:22.561409Z","iopub.execute_input":"2022-11-22T04:34:22.562019Z","iopub.status.idle":"2022-11-22T04:34:22.68473Z","shell.execute_reply.started":"2022-11-22T04:34:22.561976Z","shell.execute_reply":"2022-11-22T04:34:22.683704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We could see Rando Forest Classifier has the higherst average score, now we continue to tune and make it as a pipeline after hyperparameter tuning.**","metadata":{}},{"cell_type":"code","source":"print('Time Spent:', time.time() - start)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T04:35:36.870078Z","iopub.execute_input":"2022-11-22T04:35:36.870515Z","iopub.status.idle":"2022-11-22T04:35:37.160065Z","shell.execute_reply.started":"2022-11-22T04:35:36.870474Z","shell.execute_reply":"2022-11-22T04:35:37.159169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Chosen Model HyperParameters Tuning","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T20:41:04.418083Z","iopub.execute_input":"2022-11-22T20:41:04.418572Z","iopub.status.idle":"2022-11-22T20:41:04.467121Z","shell.execute_reply.started":"2022-11-22T20:41:04.41853Z","shell.execute_reply":"2022-11-22T20:41:04.465536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are going to perform hypertuning parameter by GridSearchCV in narrow ranges, especially focusing in finding the string parameters and then use Randomized GridSearchCV to expand the ranges to find best parameters over in large number of especially max_ddepth and n_estimators**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler \nfrom hyperopt import tpe, hp, fmin, STATUS_OK,Trials\nfrom hyperopt.pyll.base import scope","metadata":{"execution":{"iopub.status.busy":"2022-11-23T22:32:23.468711Z","iopub.execute_input":"2022-11-23T22:32:23.469126Z","iopub.status.idle":"2022-11-23T22:32:23.693152Z","shell.execute_reply.started":"2022-11-23T22:32:23.469093Z","shell.execute_reply":"2022-11-23T22:32:23.691979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nspace = {\n    \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n    \"n_estimators\": hp.choice(\"n_estimators\", [400, 500, 600, 800]),\n    \"max_depth\": hp.choice(\"max_depth\", [50, 100, 200]),\n    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n    \"max_features\": hp.choice(\"max_features\", ['sqrt', 'auto']),\n    \"min_samples_leaf\": hp.choice (\"min_samples_leaf\", [1, 3, 5]),\n    \"min_samples_split\": hp.choice(\"min_samples_split\", [2, 3, 4])\n    \n}\n\ndef hyperparameter_tuning(params):\n    rf = RandomForestClassifier(**params,n_jobs=-1, verbose = 0)\n    rf.fit(X_train, y_train)\n    y_scores = rf.predict_proba(X_test)\n    roc= metrics.roc_auc_score(y_test, y_scores[:,1])\n    return {\"loss\": -roc, \"status\": STATUS_OK}\n\n\ntrials = Trials()\n\nbest = fmin(\n    fn=hyperparameter_tuning,\n    space = space, \n    algo=tpe.suggest, \n    max_evals=5, \n    trials=trials\n)\n\nprint(\"Best: {}\".format(best))","metadata":{"execution":{"iopub.status.busy":"2022-11-23T22:32:23.91653Z","iopub.execute_input":"2022-11-23T22:32:23.91694Z","iopub.status.idle":"2022-11-23T22:36:32.047538Z","shell.execute_reply.started":"2022-11-23T22:32:23.916892Z","shell.execute_reply":"2022-11-23T22:36:32.046376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nrf = RandomForestClassifier()\nparam_grid = {'bootstrap': [True, False],\n              'criterion': ['entropy', 'gini'],\n              'max_depth': [50, 100, 150],\n              'max_features': ['auto', 'sqrt'],\n              'min_samples_leaf': [1, 3, 5],\n              'min_samples_split': [2, 4],\n              'n_estimators': [100, 200]\n             }\n\nRF_clf = GridSearchCV(estimator = rf,\n                      param_grid = param_grid,\n                      scoring = 'roc_auc',\n                      cv= 2,\n                      verbose=1,\n                      n_jobs = -1)\n\nRF_clf.fit(X_train, y_train)\nprint('Best parameters found:\\n', RF_clf.best_params_)\n\nprint('Time Spent:', time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2022-11-23T23:50:45.955543Z","iopub.execute_input":"2022-11-23T23:50:45.956002Z","iopub.status.idle":"2022-11-24T00:17:56.482637Z","shell.execute_reply.started":"2022-11-23T23:50:45.955962Z","shell.execute_reply":"2022-11-24T00:17:56.481223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nrf = RandomForestClassifier()\nparam_grid = {'bootstrap': [False],\n              'criterion': ['entropy'],\n              'max_depth': range(100, 300, 50),\n              'max_features': ['auto'],\n              'min_samples_leaf': [1, 3],\n              'min_samples_split': range(1, 11, 2),\n              'n_estimators': range(200, 1000, 50)\n             }\n\nRF_clf = RandomizedSearchCV(estimator = rf,\n                            n_iter = 30,\n                            param_distributions = param_grid,\n                            scoring = 'roc_auc',\n                            cv= 2,\n                            verbose=2,\n                            n_jobs = -1)\n\nRF_clf.fit(X_train, y_train)\nprint('Best parameters found:\\n', RF_clf.best_params_)\n\nprint('Time Spent:', time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T00:32:19.120751Z","iopub.execute_input":"2022-11-24T00:32:19.122295Z","iopub.status.idle":"2022-11-24T00:55:48.046607Z","shell.execute_reply.started":"2022-11-24T00:32:19.122235Z","shell.execute_reply":"2022-11-24T00:55:48.045143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nrf_clf = RandomForestClassifier(bootstrap = False,\n                                criterion= 'entropy',\n                                max_depth= 200,\n                                max_features= 'auto',\n                                min_samples_leaf= 1,\n                                min_samples_split= 5,\n                                n_estimators= 650)\nrf_clf.fit(X_train, y_train)\ny_pred = rf_clf.predict(X_test)\nacc = [metrics.accuracy_score(y_pred, y_test)]\nrec = [metrics.recall_score(y_pred, y_test, average=\"weighted\")]\nprec = [metrics.precision_score(y_pred, y_test, average=\"weighted\")]\nf1 = [metrics.f1_score(y_pred, y_test, average=\"weighted\")]\n    \nprint(\"accuracy    = {:.4f}\", acc)\nprint(\"recall      = {:.4f}\", rec)\nprint(\"precision   = {:.4f}\", prec)\nprint(\"f1          = {:.4f}\", f1)\n\ny_scores = rf_clf.predict_proba(X_test)\n\n# calculate ROC curve\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores[:,1])\nRFScore = metrics.auc(fpr, tpr)\nprint(RFScore)\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve- Random Forest Optimized')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-24T02:47:20.842518Z","iopub.execute_input":"2022-11-24T02:47:20.842988Z","iopub.status.idle":"2022-11-24T02:50:54.905644Z","shell.execute_reply.started":"2022-11-24T02:47:20.842951Z","shell.execute_reply":"2022-11-24T02:50:54.904328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PipeLine","metadata":{}},{"cell_type":"code","source":"# loading train_transaction data\ntt = pd.read_csv('../input/ieeecis-fraud-detection/train_transaction.csv')\nprint(tt.shape)\ntt = reduce_mem_usage(tt)\n\n# loading train_transaction data\nti = pd.read_csv('../input/ieeecis-fraud-detection/train_identity.csv')\nprint(ti.shape)\nti = reduce_mem_usage(ti)\n\ntrain = pd.merge(tt, ti, how = 'left')\nprint('Train shape',train.shape)\n\ntrain.head()\n\n\ndel tt, ti, X, y","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:37:35.802004Z","iopub.execute_input":"2022-11-25T05:37:35.802451Z","iopub.status.idle":"2022-11-25T05:40:06.510914Z","shell.execute_reply.started":"2022-11-25T05:37:35.802418Z","shell.execute_reply":"2022-11-25T05:40:06.509398Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"(590540, 394)\nMemory usage of dataframe is 1775.15 MB\nMemory usage after optimization is: 487.16 MB\nDecreased by 72.6%\n(144233, 41)\nMemory usage of dataframe is 45.12 MB\nMemory usage after optimization is: 10.00 MB\nDecreased by 77.8%\nTrain shape (590540, 434)\n","output_type":"stream"}]},{"cell_type":"code","source":"def ColumnsDropper(df):\n    mv = df.isnull().sum()/len(df)\n    df = df.drop(columns=mv[mv>0.8].index)\n    return df\n\ntrain = Devices(train)\ntrain = ColumnsDropper(train)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:40:06.513444Z","iopub.execute_input":"2022-11-25T05:40:06.513833Z","iopub.status.idle":"2022-11-25T05:40:44.782435Z","shell.execute_reply.started":"2022-11-25T05:40:06.513802Z","shell.execute_reply":"2022-11-25T05:40:44.780714Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"y, TransactionID, TransactionDT = train['isFraud'], train['TransactionID'], train['TransactionDT']\nX = train.drop(['isFraud', 'TransactionID', 'TransactionDT'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:40:44.784482Z","iopub.execute_input":"2022-11-25T05:40:44.784907Z","iopub.status.idle":"2022-11-25T05:40:48.873379Z","shell.execute_reply.started":"2022-11-25T05:40:44.78487Z","shell.execute_reply":"2022-11-25T05:40:48.871662Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"num_df = X_train.select_dtypes(include=np.number).columns\n# Numerical Preprocessing\nnum_pre = Pipeline(steps =\n                   [(\"Num Imputer\", SimpleImputer(missing_values=np.nan, strategy='median')),\n                    (\"Scaler\", preprocessing.MinMaxScaler()),\n                    (\"PCA\", PCA(n_components = 0.95))])\n\n\ncat_df = X_train.select_dtypes(exclude=np.number).columns\n# Categorical Preprocessing\ncat_pre = Pipeline(steps =\n                   [(\"Cat Imputer\", SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n                    (\"onehot\", OneHotEncoder(handle_unknown='ignore')),\n                    (\"Categorical_Selector\", SelectFromModel(ExtraTreesClassifier(n_estimators=100,\n                                                                       criterion = 'entropy')))])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:40:48.87713Z","iopub.execute_input":"2022-11-25T05:40:48.877558Z","iopub.status.idle":"2022-11-25T05:40:50.115427Z","shell.execute_reply.started":"2022-11-25T05:40:48.877521Z","shell.execute_reply":"2022-11-25T05:40:50.114008Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_pre, num_df),\n        ('cat', cat_pre, cat_df)])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:40:50.117288Z","iopub.execute_input":"2022-11-25T05:40:50.118265Z","iopub.status.idle":"2022-11-25T05:40:50.125378Z","shell.execute_reply.started":"2022-11-25T05:40:50.118226Z","shell.execute_reply":"2022-11-25T05:40:50.12375Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"\npipe = Pipeline(steps = \n                [('preprocessor', preprocessor),\n                 #('UnderSampling', RandomUnderSampler(random_state=42)),\n                 ('RandomForest', RandomForestClassifier())])\n\npipe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:40:50.127539Z","iopub.execute_input":"2022-11-25T05:40:50.128084Z","iopub.status.idle":"2022-11-25T06:11:46.247248Z","shell.execute_reply.started":"2022-11-25T05:40:50.128026Z","shell.execute_reply":"2022-11-25T06:11:46.245753Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('Num '\n                                                                   'Imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('Scaler',\n                                                                   MinMaxScaler()),\n                                                                  ('PCA',\n                                                                   PCA(n_components=0.95))]),\n                                                  Index(['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2',\n       'dist1', 'C1', 'C2',\n       ...\n       'id_01', 'id_02', 'id_05', 'id_06', 'id_11', 'id_13', 'id_17', 'id_1...\n                                                                   SelectFromModel(estimator=ExtraTreesClassifier(criterion='entropy')))]),\n                                                  Index(['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1',\n       'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15',\n       'id_16', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38',\n       'DeviceType', 'device_name', 'browser_id_31', 'version_id_31'],\n      dtype='object'))])),\n                ('RandomForest', RandomForestClassifier())])"},"metadata":{}}]},{"cell_type":"code","source":"y_hat = pipe.predict(X_test)\ny_scores = pipe.predict_proba(X_test)\nprint(metrics.confusion_matrix(y_hat, y_test))\nprint(metrics.accuracy_score(y_test, y_hat))\nprint(metrics.precision_score(y_test, y_hat, average='weighted'))\nprint(metrics.recall_score(y_test, y_hat, average='weighted'))\nprint(metrics.f1_score(y_test, y_hat, average='weighted'))\nprint(metrics.roc_auc_score(y_test, y_scores[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T06:11:46.249037Z","iopub.execute_input":"2022-11-25T06:11:46.249541Z","iopub.status.idle":"2022-11-25T06:12:04.456714Z","shell.execute_reply.started":"2022-11-25T06:11:46.249492Z","shell.execute_reply":"2022-11-25T06:12:04.455695Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"[[142215   3442]\n [   115   1863]]\n0.9759067971686931\n0.9751291335703876\n0.9759067971686931\n0.9705428715160642\n0.9126810369471645\n","output_type":"stream"}]},{"cell_type":"code","source":"param_grid = {'preprocessor__num__PCA__n_components': [0.99, 0.97, 0.95, 0.93],\n              'preprocessor__cat__Categorical_Selector__ExtraTreesClassifier__n_estimators': [100, 150, 200],\n              'RandomForest__bootstrap': [False],\n              'RandomForest__criterion': ['entropy'],\n              'RandomForest__max_depth': range(100, 300, 100),\n              'RandomForest__max_features': ['auto'],\n              'RandomForest__min_samples_leaf': [1, 3],\n              'RandomForest__min_samples_split': range(1, 10, 2),\n              'RandomForest__n_estimators': range(200, 1000, 100)\n             }\n\ngrid_search = GridSearchCV(estimator=pipe,\n                           scoring=\"roc_auc\",\n                           param_grid= param_grid,\n                           cv=2,\n                           verbose = 1,\n                           n_jobs = -1\n                          )\n\ngrid_search.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T06:58:57.498898Z","iopub.execute_input":"2022-11-25T06:58:57.499396Z","iopub.status.idle":"2022-11-25T06:58:59.951433Z","shell.execute_reply.started":"2022-11-25T06:58:57.49935Z","shell.execute_reply":"2022-11-25T06:58:59.94951Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 1920 candidates, totalling 3840 fits\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 668, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\", line 188, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/base.py\", line 258, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\", line 271, in set_params\n    self._set_params(\"_transformers\", **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/base.py\", line 258, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\", line 188, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/base.py\", line 258, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/base.py\", line 248, in set_params\n    \"with `estimator.get_params().keys()`.\" % (key, self)\nValueError: Invalid parameter ExtraTreesClassifier for estimator SelectFromModel(estimator=ExtraTreesClassifier(criterion='entropy')). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3278747473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                           )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Invalid parameter ExtraTreesClassifier for estimator SelectFromModel(estimator=ExtraTreesClassifier(criterion='entropy')). Check the list of available parameters with `estimator.get_params().keys()`."],"ename":"ValueError","evalue":"Invalid parameter ExtraTreesClassifier for estimator SelectFromModel(estimator=ExtraTreesClassifier(criterion='entropy')). Check the list of available parameters with `estimator.get_params().keys()`.","output_type":"error"}]},{"cell_type":"code","source":"grid_search.best_params_\ngrid_search.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_classifier = grid_search.best_estimator_\ny_hat = best_classifier.predict(X_test)\ny_scores = best_classifier.predict_proba(X_test)\n\nprint(metrics.confusion_matrix(y_hat, y_test))\nprint(metrics.accuracy_score(y_test, y_hat))\nprint(metrics.precision_score(y_test, y_hat, average='weighted'))\nprint(metrics.recall_score(y_test, y_hat, average='weighted'))\nprint(metrics.f1_score(y_test, y_hat, average='weighted'))\nprint(metrics.roc_auc_score(y_test, y_scores[:,1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(X, y)\nX = pd.DataFrame(X_resampled, columns = X.columns)\ny = pd.DataFrame(y_resampled)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","metadata":{},"execution_count":null,"outputs":[]}]}